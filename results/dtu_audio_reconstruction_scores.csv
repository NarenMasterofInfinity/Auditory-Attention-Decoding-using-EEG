
Model/Approach,Paper (Year),Model Type,Subject Approach,Dataset,Pearson r (DTU),Notes
ADT network (Auditory Decoding Transformer),"Liu et al., Trends in Hearing (2024)",Transformer-based network,Subject-independent (evaluated on unseen DTU subjects),DTU speech envelope reconstruction,0.167,"Average reconstruction score on DTU dataset; significantly higher than baseline models such as VLAAI, FCNN, and linear modelhttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001."
ADT w/o masking,"Liu et al., Trends in Hearing (2024)",ADT variant without anticausal masking,Subject-independent,DTU speech envelope reconstruction,0.154,ADT variant without masking; performance slightly lower than ADT with anticausal masking but close to HappyQuokkahttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=The%20average%20reconstruction%20score%20of,.01.
ADT with causal masking,"Liu et al., Trends in Hearing (2024)",ADT variant with causal masking,Subject-independent,DTU speech envelope reconstruction,0.118,ADT variant with causal masking; lower performance than ADT without masking and other modelshttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=The%20average%20reconstruction%20score%20of,.01.
HappyQuokka,"Basal et al., ICASSP (2023) summarized in Liu et al. (2024)",Transformer with auxiliary global conditioner,Subject-independent,DTU speech envelope reconstruction,0.152,Included in the ADT paper for comparison; original HappyQuokka system used for Auditory EEG Challengehttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001.
CNN,"Basal et al., ICASSP (2023) summarized in Liu et al. (2024)",Convolutional neural network,Subject-independent,DTU speech envelope reconstruction,0.151,Convolutional baseline from earlier studies; included in ADT paper for comparisonhttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001.
VLAAI (baseline as evaluated in ADT paper),"Accou et al., Scientific Reports (2023) summarized in Liu et al. (2024)",Very Large Augmented Auditory Inference network,Subject-independent,DTU speech envelope reconstruction,0.107,Baseline performance of VLAAI when re-evaluated in the ADT paperhttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001.
FCNN,"Liu et al., Trends in Hearing (2024)",Fully connected neural network,Subject-independent,DTU speech envelope reconstruction,0.097,Baseline FCNN model used for comparison in ADT paperhttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001.
Linear model,"Wong et al., Scientific Reports (2023) and Liu et al., Trends in Hearing (2024)",Linear decoder,Subject-independent,DTU speech envelope reconstruction,0.09,Linear baseline included in the ADT paperhttps://pmc.ncbi.nlm.nih.gov/articles/PMC11489951/#:~:text=Figure%206%20displays%20the%20reconstruction,.001.
VLAAI (original subject-independent),"Accou et al., Scientific Reports (2023)",Very Large Augmented Auditory Inference network,Subject-independent,DTU speech envelope reconstruction (single-speaker trials),0.17,"Median Pearson correlation on the DTU dataset; performance decreased from 0.19 to 0.17 compared to the single-speaker stories dataset, with a 61% relative improvement over the linear decoderhttps://pmc.ncbi.nlm.nih.gov/articles/PMC9842721/#:~:text=subjects%20of%20the%20publicly%20available,the%20holdout%20and%20DTU%20datasets. The linear baseline can be approximated as ~0.106 (0.17/1.61)."
Linear model (approx baseline for VLAAI),"Accou et al., Scientific Reports (2023)",Linear decoder,Subject-independent,DTU speech envelope reconstruction (approximate),0.106,"Approximate linear baseline derived from the 61% relative performance increase reported for the VLAAI networkhttps://pmc.ncbi.nlm.nih.gov/articles/PMC9842721/#:~:text=dataset%20compared%20to%20the%20test,0.01%29%2C%20with%20a. The ADT paper reports a linear model correlation of 0.090, showing variation across studies."
